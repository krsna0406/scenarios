"""
code snippet:
dates = ['2019-07-01 12:01:19.111', '2019-06-24 12:01:19.222', '2019-11-16 16:44:55.406', '2019-11-16 16:50:59.406']
df = spark.createDataFrame(list( zip(dates)), schema=schema)

explanation:
dataset:

dates = [
    '2019-07-01 12:01:19.111',
    '2019-06-24 12:01:19.222',
    '2019-11-16 16:44:55.406',
    '2019-11-16 16:50:59.406'
]


Spark needs data like:

Row1: ('2019-07-01 12:01:19.111',)
Row2: ('2019-06-24 12:01:19.222',)
Row3: ('2019-11-16 16:44:55.406',)
Row4: ('2019-11-16 16:50:59.406',)


 df = spark.createDataFrame(list(zip(dates)), schema=schema)
zip() is used because createDataFrame() expects data in row format (list/tuple per row).

analysis:

Your list is:

dates = [
    '2019-07-01 12:01:19.111',
    '2019-06-24 12:01:19.222',
    '2019-11-16 16:44:55.406',
    '2019-11-16 16:50:59.406'
]


Spark needs data like:

Row1: ('2019-07-01 12:01:19.111',)
Row2: ('2019-06-24 12:01:19.222',)
Row3: ('2019-11-16 16:44:55.406',)
Row4: ('2019-11-16 16:50:59.406',)


zip(dates) converts:

['a','b','c']


to:

[('a',), ('b',), ('c',)]


This makes each element a tuple â†’ a single column row.

>>
Without zip, it fails

If you try:

spark.createDataFrame(dates)


Spark sees a string as iterable â†’ breaks into characters:

'2019-07-01...' â†’ '2','0','1','9','-','0','7',...


Your DataFrame becomes corrupted.


ðŸ”¥ Simpler Alternative (no zip)  >>

You can also do:

df = spark.createDataFrame([(d,) for d in dates], ["date"])


Or:

df = spark.createDataFrame(dates, "string").toDF("date")


âœ” SUMMARY
PURPOSE	WHY USED
ZIP()	CONVERTS A LIST INTO ROW-FORMAT (TUPLE ROWS)
NEEDED BECAUSE	SPARK EXPECTS EACH ROW AS A TUPLE/LIST

"""

print(__doc__)

import sys
import os
import urllib.request
import ssl

from pyspark.sql.types import IntegerType

python_path = sys.executable
os.environ['PYSPARK_PYTHON'] = python_path

os.environ['HADOOP_HOME'] = r'C:\app\zeyoplus\soft\sw\hadoop'
os.environ['JAVA_HOME'] = r'C:\Users\Krishna\.jdks\ms-17.0.16'        #  <----- ðŸ”´JAVA PATHðŸ”´
######################ðŸ”´ðŸ”´ðŸ”´################################

from pyspark import SparkContext,SparkConf
from pyspark.sql import  SparkSession

from pyspark.sql.functions import *

from pyspark.sql import Window

conf=SparkConf().setAppName("s1").setMaster("local[*]")
sc=SparkContext(conf=conf)
spark=SparkSession.builder.getOrCreate()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[1]") \
    .appName('SparkByExamples.com') \
    .getOrCreate()



data=["abc","xyz"]
schema=["column1"]
df=spark.createDataFrame(list(zip(data)),schema=schema)
df.show()

# print(list(zip(data)))